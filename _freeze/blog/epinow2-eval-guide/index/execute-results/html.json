{
  "hash": "eafcdd1bab62d0c8491426e4b772c28b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to Validate Time-Varying Reproduction Number Estimation in EpiNow2: MCMC Diagnostics, Convergence, and Forecast Evaluation\"\nauthor:\n  - name: \"James Mba Azam\"\n  - orcid: \"0000-0001-5782-7330\"\nbibliography: index.bib\ndate: \"2025-11-08\"\nformat:\n  html:\n    toc: true\nexecute:\n  eval: true\n  cache: true\neditor_options:\n  chunk_output_type: console\nfreeze: true\ncategories: [forecasting, EpiNow2, Bayesian Analysis, Reproduction numbers, R, Stan]\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EpiNow2)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(data.table)\nlibrary(scoringutils)\nlibrary(dplyr)\nlibrary(ggplot2)\n```\n:::\n\n\n\n## Introduction\n\nThis is a comprehensive guide on validating time-varying reproduction number (Rt) estimates from EpiNow2 model runs. EpiNow2 is a Bayesian framework for estimating Rt—a key epidemiological metric indicating disease transmission dynamics—and generating forecasts of case counts. Rigorous validation of these estimates is essential for reliable inference and evidence-based decision-making in infectious disease surveillance and outbreak response.\n\n::: {.callout-note}\n### What is the Reproduction Number (Rt)?\n\nThe **time-varying reproduction number** (Rt) represents the average number of secondary infections caused by an infected individual at time t. It's a critical metric for understanding epidemic dynamics:\n\n- **Rt > 1**: Epidemic is growing (each case generates more than one new case)\n- **Rt = 1**: Epidemic is stable (endemic equilibrium)\n- **Rt < 1**: Epidemic is declining (heading toward extinction)\n\nUnlike the basic reproduction number (R0), which assumes a fully susceptible population, Rt accounts for changing conditions like interventions, behavior changes, and population immunity.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n### EpiNow2 uses a [Stan](https://mc-stan.org/) backend\n\n`{EpiNow2}` uses [Stan](https://mc-stan.org/) in the backend for model specification, fitting, and inference.\n\nThis means that the model fit can be assessed using the same tools and techniques used for any Stan model. The EpiNow2 package provides a convenient interface to run the models, but the underlying model is a Stan model.\n\nThe returned `fit` object is either a [`<stanfit>`](https://mc-stan.org/rstan/reference/stanfit-class.html) or [`CmdStanModel`](https://mc-stan.org/cmdstanr/reference/CmdStanModel.html) object, depending on the backend ([`{rstan}`](https://mc-stan.org/rstan/index.html) or [`{cmdstanr}`](https://mc-stan.org/cmdstanr/index.html)) used. For example, you can use the [`{bayesplot}`](https://mc-stan.org/bayesplot/reference/bayesplot-package.html) and [`{posterior}`](https://mc-stan.org/posterior/) packages to visualize and summarize the model diagnostics.\n:::\n\nValidating EpiNow2 Rt estimates requires a three-pillar approach: (1) **MCMC diagnostics** to ensure the Bayesian sampler has converged and is sampling efficiently from the posterior distribution, (2) **convergence checks** to verify reliable parameter estimates (via metrics like Rhat and effective sample size), and (3) **forecast evaluation** to assess predictive performance against observed data using proper scoring rules. This guide demonstrates each component using the Stan ecosystem's diagnostic tools (`bayesplot`, `posterior`) and the `scoringutils` package for forecast scoring.\n\nWe'll demonstrate this workflow by running an example EpiNow2 model for Rt estimation, examining MCMC diagnostics and convergence metrics, and evaluating forecast performance using proper scoring rules.\n\n## Demonstration: EpiNow2 Model Setup and Execution\n\n### Setup: Data and epidemiological parameters\n\nTo estimate time-varying Rt, EpiNow2 requires:\n1. **Case data**: Time series of reported cases\n2. **Epidemiological delays**: Generation time, incubation period, and reporting delays\n3. **Prior distributions**: Initial beliefs about Rt and other parameters\n\nLet's set up these components:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Configure parallel processing for faster MCMC sampling\n# Use at most 4 cores, leaving 1 core free for system operations\noptions(mc.cores = min(4, parallel::detectCores() - 1))\n\n# Define epidemiological delay distributions\n# In practice, source these from literature or estimate from data\n\n# Generation time: time between successive infections in a transmission chain\n# Using Gamma distribution with uncertainty in parameters\ngeneration_time <- Gamma(\n    shape = Normal(1.3, 0.3),    # Mean of ~3.5 days\n    rate = Normal(0.37, 0.09),   # Moderate spread\n    max = 14                      # Maximum generation interval\n)\n\n# Incubation period: time from infection to symptom onset\n# Using LogNormal distribution (common for incubation periods)\nincubation_period <- LogNormal(\n    meanlog = Normal(1.6, 0.06), # Median ~5 days\n    sdlog = Normal(0.4, 0.07),   # Variability in incubation\n    max = 14                      # Maximum incubation period\n)\n\n# Reporting delay: time from symptom onset to case reporting\n# Accounts for testing, lab processing, and reporting lags\nreporting_delay <- LogNormal(mean = 2, sd = 1, max = 10)\n\n# Load case data\n# Using first 60 days of example data to demonstrate workflow\n# In practice, use your complete outbreak data\nreported_cases <- example_confirmed[1:60]\n\n# Visualize the input data\ncases_plots <- ggplot(reported_cases, aes(x = date, y = confirm)) +\n    geom_col(fill = \"steelblue\", alpha = 0.7) +\n    labs(\n      title = \"Confirmed Cases: Input Data\",\n      x = \"Date\",\n      y = \"Reported Cases\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(face = \"bold\"))\n\ncases_plots\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/model-setup-1.png){width=672}\n:::\n:::\n\n\n\n### Running the Rt estimation\n\nNow we fit the EpiNow2 model to estimate time-varying Rt and generate forecasts:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- epinow(\n    data = reported_cases,\n    generation_time = gt_opts(generation_time),\n    # Prior on initial Rt: LogNormal(mean=2, sd=0.1) suggests Rt ≈ 2\n    rt = rt_opts(prior = LogNormal(mean = 2, sd = 0.1)),\n    # Combine incubation and reporting delays\n    delays = delay_opts(incubation_period + reporting_delay)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLogging threshold set at INFO for the name logger\nWriting EpiNow2 logs to the console and:\n'/var/folders/vr/dn4r1_zj417drd1zr9301trw0000gp/T//RtmpKjo8ww/regional-epinow/2020-04-21.log'.\nLogging threshold set at INFO for the name logger\nWriting EpiNow2.epinow logs to the console and:\n'/var/folders/vr/dn4r1_zj417drd1zr9301trw0000gp/T//RtmpKjo8ww/epinow/2020-04-21.log'.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize Rt estimates and forecasts\n# This plot shows: estimated Rt over time, case nowcasts, and forecasts\nplot(out)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/run-epinow-1.png){width=672}\n:::\n:::\n\n\n\nThe plot above shows three key outputs:\n1. **Estimated Rt trajectory**: How transmission intensity changed over time\n2. **Infections by date of infection**: Nowcast accounting for reporting delays\n3. **Forecast**: Predicted future cases with uncertainty intervals\n\n## Step 1: MCMC Diagnostics and Convergence Checks\n\nBefore trusting our Rt estimates, we must verify that the Bayesian MCMC sampler converged properly. EpiNow2 uses Stan for inference, giving us access to Stan's comprehensive diagnostic toolkit via the `bayesplot` and `posterior` packages.\n\n### Key diagnostic metrics\n\nWe evaluate convergence using several complementary diagnostics:\n\n- **Divergent transitions**. These should be minimized; ideally 0. Divergent transitions can be improved by tuning Stan controls like `adapt_delta`, `max_treedepth`, and `stepsize` in `stan_opts()`. If there are divergent transitions, increase `adapt_delta` (e.g., 0.95 or 0.99). See an in‑depth explanation at <https://mc-stan.org/learn-stan/diagnostics-warnings.html>.\n- **Rhat**. These should be close to 1 indicating good mixing. Rhat values should be less than 1.05 (See `?rstan::Rhat`).\n- **Treedepth**. This should be low; high values indicate potential issues with the model.\n- **ESS values**. These should be high; low values indicate insufficient sampling. In particular, both bulk‑ESS and tail‑ESS should be at least ~100 per chain to ensure reliable posterior quantile estimates (see `?rstan::ess_bulk`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the Stan fit object from EpiNow2 output\nfit <- out$estimates$fit\n\n# Extract NUTS sampler parameters (No-U-Turn Sampler - Stan's HMC variant)\nnp <- bayesplot::nuts_params(fit)\ndivergence_data <- subset(np, Parameter == \"divergent__\")  # Sampling failures\ntreedepth_data <- subset(np, Parameter == \"treedepth__\")    # Trajectory lengths\n\n# Compute convergence metrics for all parameters\n# Rhat: measures between-chain vs within-chain variance\n# ESS: effective sample size (accounts for autocorrelation)\nposterior_summary <- posterior::summarize_draws(\n    fit, c(posterior::default_convergence_measures(), \"ess_basic\")\n) |> subset(variable != \"lp__\")  # Exclude log-probability\n\n# Extract minimum ESS values across all parameters\n# Worst-case ESS determines overall inference reliability\nfit_ess_basic <- min(posterior_summary$ess_basic, na.rm = TRUE)\nfit_ess_bulk <- min(posterior_summary$ess_bulk, na.rm = TRUE)\nfit_ess_tail <- min(posterior_summary$ess_tail, na.rm = TRUE)\n\n# Compile diagnostic summary table\ndiagnostics <- data.table(\n    \"samples\" = nrow(np) / length(unique(np$Parameter)),  # Total post-warmup samples\n    \"max_rhat\" = round(max(posterior_summary$rhat, na.rm = TRUE), 3),  # Worst Rhat\n    \"divergent_transitions\" = sum(divergence_data$Value),  # Count of divergences\n    \"per_divergent_transitions\" = mean(divergence_data$Value),  # Proportion divergent\n    \"max_treedepth\" = max(treedepth_data$Value),  # Maximum tree depth reached\n    \"ess_basic\" = fit_ess_basic,  # Basic ESS\n    \"ess_bulk\" = fit_ess_bulk,    # ESS for bulk of distribution\n    \"ess_tail\" = fit_ess_tail     # ESS for distribution tails\n)\n\n# Calculate tree depth saturation\n# High proportion indicates sampler hitting computational limits\ndiagnostics[, no_at_max_treedepth :=\n                sum(treedepth_data$Value == max_treedepth)\n][, per_at_max_treedepth := no_at_max_treedepth / samples]\n\nknitr::kable(diagnostics)\n```\n\n::: {.cell-output-display}\n\n\n| samples| max_rhat| divergent_transitions| per_divergent_transitions| max_treedepth| ess_basic| ess_bulk| ess_tail| no_at_max_treedepth| per_at_max_treedepth|\n|-------:|--------:|---------------------:|-------------------------:|-------------:|---------:|--------:|--------:|-------------------:|--------------------:|\n|    2000|     1.01|                     0|                         0|             9|  1325.989| 1277.945| 1109.327|                 837|               0.4185|\n\n\n:::\n:::\n\n\n\n### Interpreting the diagnostics\n\nThe diagnostics table above summarizes key MCMC performance metrics. For reliable Rt estimates, we should see:\n\n- **max_rhat < 1.05**: Our example shows good convergence (Rhat values close to 1.0)\n- **divergent_transitions = 0**: No divergent transitions is ideal. Any divergences suggest the sampler struggled with the posterior geometry\n- **ESS values > 400** (for 4 chains × 1000 samples): Bulk-ESS and tail-ESS should be at least 100 per chain. Higher is better for reliable inference\n- **per_at_max_treedepth < 0.01**: Few samples hitting maximum tree depth indicates efficient sampling\n\nBased on these criteria, we can assess whether our Rt estimates are trustworthy.\n\n### Visual MCMC diagnostics\n\nWhile numerical summaries are useful, visual diagnostics provide deeper insight into sampler behavior. Let's examine trace plots and rank plots for key parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract key Rt parameters for visualization\n# Using a subset of timepoints for clarity\nlibrary(bayesplot)\ncolor_scheme_set(\"mix-blue-red\")\n\n# Trace plots show MCMC sampling behavior over iterations\n# Good mixing appears as \"fuzzy caterpillars\" with chains overlapping\nmcmc_trace(fit, pars = c(\"R[1]\", \"R[15]\", \"R[30]\", \"R[45]\"),\n           facet_args = list(ncol = 2, strip.position = \"left\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/visual-diagnostics-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rank plots check for uniform distribution of ranks across chains\n# Deviations from uniform indicate convergence issues\nmcmc_rank_overlay(fit, pars = c(\"R[1]\", \"R[15]\", \"R[30]\", \"R[45]\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rank-plots-1.png){width=960}\n:::\n:::\n\n\n\nWell-mixed chains should show:\n- **Trace plots**: Overlapping \"fuzzy caterpillar\" patterns with no trends or stuck chains\n- **Rank plots**: Uniform distribution of ranks across all chains (no systematic patterns)\n\n::: {.callout-warning}\n### What if diagnostics fail?\n\nIf diagnostics reveal problems (high Rhat, low ESS, divergences):\n\n1. **Increase sampling**: Use `stan_opts(chains = 4, iter_sampling = 4000)` for more samples\n2. **Adjust sampler settings**: Increase `adapt_delta` (e.g., 0.95 or 0.99) to reduce divergences\n3. **Check model specification**: Ensure delay distributions and priors are reasonable\n4. **Increase max_treedepth**: Use `stan_opts(max_treedepth = 12)` if hitting limits\n5. **Consult Stan diagnostics guide**: <https://mc-stan.org/misc/warnings.html>\n\n**Do not use** Rt estimates with failing diagnostics—they may be severely biased!\n:::\n\n## Step 2: Forecast Performance Evaluation\n\nForecast performance can be evaluated by comparing the forecast of reported cases with the observed cases using Proper Scoring Rules[@gneiting2007strictly;@carvalho2016overview] such as the Continuous Ranked Probability Score (CRPS) and Weighted Interval Score (WIS). These are available via the `scoringutils::score()` function.\n\nThe `scoringutils::score()` function requires a dataset that contains at least the following columns:\n\n- `date`: the date of the forecast\n- `prediction`: forecast values\n- `true_value`: the observed values\n- `quantile`: If evaluating quantiles, the quantile of the forecasted values (e.g., median, lower and upper bounds), in ascending order.\n\nLet's extract and prepare the forecasts and corresponding observed cases for the evaluation.\n\nEpiNow2's `epinow()` function returns a list of model outputs in the raw and several summarised formats for various use cases. In particular, there are time series of \"infections\", \"reported_cases\", \"growth_rate\", and \"R\", all grouped into \"estimate\", \"estimate based on partial data\", and \"forecast\".\n\nHere, we're interested in the \"forecast\" type of the \"reported_cases\" variable, including the median and quantiles (lower and upper bounds).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the forecasts\nforecasts <- out$estimates$summarised[variable == \"reported_cases\"][type == \"forecast\", ] %>% \n    select(-c(mean, sd, variable, strat, type))\n```\n:::\n\n\n\nLet's also extract the corresponding subset of the observed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract observed cases for the same period\nobs_data <- example_confirmed[date >= min(forecasts$date) & date <= max(forecasts$date)]\n```\n:::\n\n\n\n### Visualizing forecasts against observations\n\nBefore scoring, let's visualize how well our forecasts align with observed data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data for plotting\nplot_data <- merge(forecasts, obs_data, by = \"date\", all.x = TRUE)\n\n# Create forecast vs observed plot\nggplot(plot_data, aes(x = date)) +\n  # Prediction intervals (90%, 50%, 20%)\n  geom_ribbon(aes(ymin = lower_90, ymax = upper_90), alpha = 0.2, fill = \"steelblue\") +\n  geom_ribbon(aes(ymin = lower_50, ymax = upper_50), alpha = 0.3, fill = \"steelblue\") +\n  geom_ribbon(aes(ymin = lower_20, ymax = upper_20), alpha = 0.4, fill = \"steelblue\") +\n  # Median forecast\n  geom_line(aes(y = median), color = \"steelblue\", size = 1) +\n  # Observed data\n  geom_point(aes(y = confirm), color = \"black\", size = 2) +\n  geom_line(aes(y = confirm), color = \"black\", linetype = \"dashed\") +\n  labs(\n    title = \"EpiNow2 Forecast vs Observed Cases\",\n    subtitle = \"Shaded regions show 20%, 50%, and 90% prediction intervals\",\n    x = \"Date\",\n    y = \"Reported Cases\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/forecast-plot-1.png){width=960}\n:::\n:::\n\n\n\nIdeally, observed values (black points) should fall mostly within the prediction intervals, especially the wider ones. Systematic deviations suggest model miscalibration.\n\n### Preparing data for scoring\n\nNow, let's combine the extracted forecasts with the observed data and clean up the quantiles into a format suitable for use with `scoringutils`. We will be cleaning them up because the `forecasts` data.table labels the quantiles as `lower_20`, `upper_20`, etc., where the number indicates the quantile level (e.g., 50% for the median), but that is not compatible with `scoringutils`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine forecasts with observed cases\neval_dt <- merge(forecasts, obs_data, by = \"date\")[, true_value := confirm][, confirm := NULL]\n\n# Melt the data to long format\ncols_to_melt <- c(\"median\", grep(\"^(lower|upper)_\", names(eval_dt), value = TRUE))\neval_long <- melt(\n    eval_dt,\n    id.vars = setdiff(names(eval_dt), cols_to_melt),\n    measure.vars = cols_to_melt,\n    variable.name = \"prediction\",\n    value.name = \"value\"\n)\n\n# Prepare the evaluation data\neval_long[, quantile := fifelse(\n  prediction == \"median\", 0.5,\n  fifelse(\n    grepl(\"^lower_\", prediction),\n    (1 - as.numeric(sub(\"lower_\", \"\", prediction)) / 100) / 2,\n    fifelse(\n      grepl(\"^upper_\", prediction),\n      (1 + as.numeric(sub(\"upper_\", \"\", prediction)) / 100) / 2,\n      NA_real_\n    )\n  )\n)][, prediction := value][, value := NULL]\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in fifelse(grepl(\"^lower_\", prediction), (1 - as.numeric(sub(\"lower_\",\n: NAs introduced by coercion\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in fifelse(grepl(\"^upper_\", prediction), (1 + as.numeric(sub(\"upper_\",\n: NAs introduced by coercion\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sort the data by quantile\nsetorder(eval_long, quantile) %>% \n    head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          date true_value prediction quantile\n        <Date>      <num>      <num>    <num>\n 1: 2020-04-22       2729    1526.00     0.05\n 2: 2020-04-23       3370    1623.90     0.05\n 3: 2020-04-24       2646    1911.95     0.05\n 4: 2020-04-25       3021    1599.00     0.05\n 5: 2020-04-26       2357    1701.95     0.05\n 6: 2020-04-27       2324    1507.00     0.05\n 7: 2020-04-28       1739    1237.85     0.05\n 8: 2020-04-22       2729    1895.00     0.25\n 9: 2020-04-23       3370    2056.75     0.25\n10: 2020-04-24       2646    2420.00     0.25\n```\n\n\n:::\n:::\n\n\n\nNow we can use the `scoringutils` package to compute and summarise the scores. By default, a range of scores will be computed, including the interval_score, dispersion, underprediction, overprediction, coverage_deviation,  bias, and ae_median.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mainly uses the scoringutils package\nscored_scores <- eval_long[, quantile_level := quantile][, quantile := NULL][, model := \"EpiNow2\"] |> # align names to scoringutils preference\n    as_forecast_quantile(observed  = \"true_value\", predicted = \"prediction\") |> \n    score() %>%\n    summarise_scores()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: ! Computation for `interval_coverage_90` failed. Error: ! To compute the\n  interval coverage for an interval range of \"90%\", the 0.05 and 0.95 quantiles\n  are required.\n```\n\n\n:::\n\n```{.r .cell-code}\nknitr::kable(scored_scores, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|model   |     wis| overprediction| underprediction| dispersion|   bias| interval_coverage_50| ae_median|\n|:-------|-------:|--------------:|---------------:|----------:|------:|--------------------:|---------:|\n|EpiNow2 | 313.898|         33.963|         150.892|    129.043| -0.143|                0.571|   444.286|\n\n\n:::\n:::\n\n\n\n## Interpretation of scores\n\nThe table above provides a summary of the forecast performance across multiple metrics. However, interpreting these scores in isolation can be challenging, as they are not relative to other models. Proper scoring rules are most informative when comparing multiple models or against baseline forecasts.\n\nLet's understand what each metric tells us:\n\n- **interval_score**: Overall forecast accuracy combining sharpness (narrow intervals) and calibration (actual coverage). Lower is better.\n- **dispersion**: Measures the width of prediction intervals. Lower values indicate sharper (more confident) forecasts.\n- **underprediction** and **overprediction**: Penalties for forecasts that systematically miss below or above observations.\n- **coverage_deviation**: Difference between nominal and empirical coverage of prediction intervals. Values near zero indicate well-calibrated intervals.\n- **bias**: Systematic tendency to over- or under-forecast. Values near zero are ideal.\n- **ae_median**: Absolute error of the median forecast. A simple point forecast accuracy metric.\n\nFor this example run, well-performing models should show low interval scores, minimal bias, and coverage deviation close to zero\n\n::: {.callout-tip}\n\n### Comparing against a baseline model\n\nA common practice is to have a baseline model [@stapper2025mind], such as a naive model, and compare the scores of your model against it.\n:::\n\n## Conclusion: Validation Checklist\n\nWe've demonstrated a comprehensive three-pillar approach to validating EpiNow2 Rt estimates. Use this checklist to assess whether your model results are reliable:\n\n### ✓ MCMC Diagnostics Pass\n- [ ] **Rhat < 1.05** for all parameters (ideally < 1.01)\n- [ ] **Zero divergent transitions** (or < 1% if unavoidable)\n- [ ] **ESS bulk and tail > 400** total (>100 per chain minimum)\n- [ ] **Low tree depth saturation** (< 1% hitting max_treedepth)\n- [ ] **Trace plots** show good mixing (fuzzy caterpillars)\n- [ ] **Rank plots** show uniform distribution\n\n### ✓ Forecast Evaluation Complete\n- [ ] **Visual inspection** shows forecasts align with observations\n- [ ] **Prediction intervals** contain observed values at appropriate rates\n- [ ] **Scoring metrics** calculated (interval score, bias, coverage)\n- [ ] **Comparison to baseline** (if available) shows improvement\n\n### ✓ Interpretation and Documentation\n- [ ] Rt trajectory makes epidemiological sense\n- [ ] Uncertainty intervals are appropriate (not too narrow/wide)\n- [ ] Model assumptions documented (delays, priors)\n- [ ] Limitations acknowledged\n\n**When to trust your Rt estimates**: Ideally, all diagnostic checks should pass and forecast evaluation should be satisfactory. However, in practice, minor diagnostic issues (e.g., slightly elevated Rhat < 1.1, moderate ESS values) may be acceptable if you're aware of the limitations and interpret results with appropriate caution. Serious issues (divergent transitions, Rhat > 1.1, very low ESS < 100) require investigation and likely model refinement. Document any diagnostic concerns and consider their implications for your conclusions.\n\n**When to seek help**: If diagnostics consistently fail despite tuning, or forecasts systematically miss observations, consult the Stan community forums or EpiNow2 GitHub discussions.\n\n## Future updates\n\nIn future updates, I will explore more advanced techniques for model diagnostics and evaluation, including posterior predictive checks, cross-validation, and more using packages like [`loo`](https://mc-stan.org/loo/), [shinystan](https://mc-stan.org/shinystan/), and `bayesplot`\n\n## Resources\n\nFor more in-depth information on advanced model diagnostics and forecast evaluation techniques, I recommend the following resources:\n\n### MCMC Diagnostics and Stan\n\n- **[Stan Diagnostics Guide](https://mc-stan.org/learn-stan/diagnostics-warnings.html)**: Official guide on diagnosing and resolving MCMC convergence issues, including detailed explanations of divergences, Rhat, and ESS.\n\n- **[bayesplot Visual MCMC Diagnostics](https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html)**: Comprehensive tutorial on using trace plots, rank plots, and other visual diagnostics to assess MCMC performance.\n\n- **[Posterior Predictive Checks](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html)**: Guide to validating model fit by comparing simulated data from the posterior to observed data.\n\n### Forecast Evaluation\n\n- **[Understanding, Evaluating, and Improving Forecasts of Infectious Disease Burden](https://nfidd.github.io/ueifid/)**: Open course covering forecast methodology, proper scoring rules, and evaluation best practices specific to infectious disease applications.\n\n- **[scoringutils R package](https://epiforecasts.io/scoringutils/)**: Documentation for computing and visualizing forecast scores (CRPS, WIS, interval score, etc.) with practical examples.\n\n- **[scoringRules R package](https://cran.r-project.org/web/packages/scoringRules/vignettes/article.pdf)**: Theoretical foundations and implementations of proper scoring rules for probabilistic forecasts.\n\n### EpiNow2 Documentation\n\n- **[EpiNow2 package website](https://epiforecasts.io/EpiNow2/)**: Official documentation with function references, vignettes, and usage examples.\n\n- **[EpiNow2 GitHub](https://github.com/epiforecasts/EpiNow2)**: Source code, issue tracker, and community discussions for troubleshooting and feature requests.\n\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}