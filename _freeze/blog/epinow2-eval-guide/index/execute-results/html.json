{
  "hash": "75f0d053fe0d432e479d2ee5786536b5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to evaluate the 'goodness of fit' of an EpiNow2 model run\"\nauthor:\n  - name: \"James Mba Azam\"\n  - orcid: \"0000-0001-5782-7330\"\nbibliography: index.bib\ndate: \"2025-11-08\"\nformat:\n  html:\n    theme:\n      light: lux\n      dark: cyborg\n    toc: true\nexecute:\n  eval: true\n  cache: true\neditor_options:\n  chunk_output_type: console\nfreeze: true\ncategories: [forecasting, EpiNow2, Bayesian Analysis, Reproduction numbers, R, Stan]\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EpiNow2)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(data.table)\nlibrary(scoringutils)\nlibrary(dplyr)\nlibrary(ggplot2)\n```\n:::\n\n\n\n## Introduction\n\nThis is a quick guide on how to assess or evaluate the output of an EpiNow2 model run.\n\n::: {.callout-tip collapse=\"true\"}\n\n### EpiNow2 uses a [Stan](https://mc-stan.org/) backend\n\n`{EpiNow2}` uses [Stan](https://mc-stan.org/) in the backend for model specification, fitting, and inference.\n\nThis means that the model fit can be assessed using the same tools and techniques used for any Stan model. The EpiNow2 package provides a convenient interface to run the models, but the underlying model is a Stan model.\n\nThe returned `fit` object is either a [`<stanfit>`](https://mc-stan.org/rstan/reference/stanfit-class.html) or [`CmdStanModel`](https://mc-stan.org/cmdstanr/reference/CmdStanModel.html) object, depending on the backend ([`{rstan}`](https://mc-stan.org/rstan/index.html) or [`{cmdstanr}`](https://mc-stan.org/cmdstanr/index.html)) used. For example, you can use the [`{bayesplot}`](https://mc-stan.org/bayesplot/reference/bayesplot-package.html) and [`{posterior}`](https://mc-stan.org/posterior/) packages to visualize and summarize the model diagnostics.\n:::\n\nEvaluating or assessing an `{EpiNow2}` model fit requires a holistic approach involving evaluating the MCMC diagnostics as well as the performance of the forecast against observed data. We will therefore proceed in that regard here.\n\nLet's start by setting up and running an example model, assessing the MCMC diagnostics, and then evaluating the forecast performance.\n\n## Running an example model\n\n### Input preparation\n\nThis will involve: loading required packages, setting up the model parameters, and running the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set number of cores for parallel processing.\noptions(mc.cores = min(4, parallel::detectCores() - 1))\n\n\n# Set example generation time, incubation period, and reporting delay. In practice, these should use estimates from the literature or be estimated from data.\ngeneration_time <- Gamma(\n    shape = Normal(1.3, 0.3),\n    rate = Normal(0.37, 0.09),\n    max = 14\n)\n\nincubation_period <- LogNormal(\n    meanlog = Normal(1.6, 0.06),\n    sdlog = Normal(0.4, 0.07),\n    max = 14\n)\nreporting_delay <- LogNormal(mean = 2, sd = 1, max = 10)\n\n# Use example case data.\n\nreported_cases <- example_confirmed[1:60]\n\n# Plot the data\ncases_plots <- ggplot(reported_cases, aes(x = date, y = confirm)) +\n    geom_col() +\n    labs(title = \"Confirmed cases\", x = \"Date\", y = \"Cases\") +\n    theme_minimal()\n\ncases_plots\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/model-setup-1.png){width=672}\n:::\n:::\n\n\n\n### Fitting the model\n\nEstimate Rt and nowcast/forecast cases by date of infection.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- epinow(\n    data = reported_cases,\n    generation_time = gt_opts(generation_time),\n    rt = rt_opts(prior = LogNormal(mean = 2, sd = 0.1)),\n    delays = delay_opts(incubation_period + reporting_delay)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLogging threshold set at INFO for the name logger\nWriting EpiNow2 logs to the console and:\n'/var/folders/vr/dn4r1_zj417drd1zr9301trw0000gp/T//RtmpMm2rHx/regional-epinow/2020-04-21.log'.\nLogging threshold set at INFO for the name logger\nWriting EpiNow2.epinow logs to the console and:\n'/var/folders/vr/dn4r1_zj417drd1zr9301trw0000gp/T//RtmpMm2rHx/epinow/2020-04-21.log'.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the model fit\nplot(out)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/run-epinow-1.png){width=672}\n:::\n:::\n\n\n\n## Assessing model diagnostics\n\nWe'll first extract the Stan fit object and compute diagnostics. The `epinow()` function returns a list of model outputs, including the Stan fit object. This is what we need to assess the model fit. Stan provides diagnostic metrics to assess the model fit, convergence, etc. which we'll use to assess the model fit. The Stan ecosystem has a rich set of tools for diagnosing model fit, including the `bayesplot` and `posterior` packages. These tools provide a way to visualize and summarize the model diagnostics. For an indepth look at MCMC diagnostics, for example, see the bayesplot R package vignette on [visual MCMC diagnostics](https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html). You can also do [posterior predictive checks](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html) using the `bayesplot` package.\n\nDiagnostics include:\n\n- **Divergent transitions**. These should be minimized; ideally 0. Divergent transitions can be improved by tuning Stan controls like `adapt_delta`, `max_treedepth`, and `stepsize` in `stan_opts()`. If there are divergent transitions, increase `adapt_delta` (e.g., 0.95 or 0.99). See an in‑depth explanation at <https://mc-stan.org/learn-stan/diagnostics-warnings.html>.\n- **Rhat**. These should be close to 1 indicating good mixing. Rhat values should be less than 1.05 (See `?rstan::Rhat`).\n- **Treedepth**. This should be low; high values indicate potential issues with the model.\n- **ESS values**. These should be high; low values indicate insufficient sampling. In particular, both bulk‑ESS and tail‑ESS should be at least ~100 per chain to ensure reliable posterior quantile estimates (see `?rstan::ess_bulk`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- out$estimates$fit\n\nnp <- bayesplot::nuts_params(fit)\ndivergence_data <- subset(np, Parameter == \"divergent__\")\ntreedepth_data <- subset(np, Parameter == \"treedepth__\")\n\nposterior_summary <- posterior::summarize_draws(\n    fit, c(posterior::default_convergence_measures(), \"ess_basic\")\n) |> subset(variable != \"lp__\")\n\nfit_ess_basic <- min(posterior_summary$ess_basic, na.rm = TRUE)\nfit_ess_bulk <- min(posterior_summary$ess_bulk, na.rm = TRUE)\nfit_ess_tail <- min(posterior_summary$ess_tail, na.rm = TRUE)\n\ndiagnostics <- data.table(\n    \"samples\" = nrow(np) / length(unique(np$Parameter)),\n    \"max_rhat\" = round(max(posterior_summary$rhat, na.rm = TRUE), 3),\n    \"divergent_transitions\" = sum(divergence_data$Value),\n    \"per_divergent_transitions\" = mean(divergence_data$Value),\n    \"max_treedepth\" = max(treedepth_data$Value),\n    \"ess_basic\" = fit_ess_basic,\n    \"ess_bulk\" = fit_ess_bulk,\n    \"ess_tail\" = fit_ess_tail\n)\n\ndiagnostics[, no_at_max_treedepth :=\n                sum(treedepth_data$Value == max_treedepth)\n][, per_at_max_treedepth := no_at_max_treedepth / samples]\n\nknitr::kable(diagnostics)\n```\n\n::: {.cell-output-display}\n\n\n| samples| max_rhat| divergent_transitions| per_divergent_transitions| max_treedepth| ess_basic| ess_bulk| ess_tail| no_at_max_treedepth| per_at_max_treedepth|\n|-------:|--------:|---------------------:|-------------------------:|-------------:|---------:|--------:|--------:|-------------------:|--------------------:|\n|    2000|     1.01|                     0|                         0|             9|  1325.989| 1277.945| 1109.327|                 837|               0.4185|\n\n\n:::\n:::\n\n\n\n## Evaluating forecast performance\n\nForecast performance can be evaluated by comparing the forecast of reported cases with the observed cases using Proper Scoring Rules[@gneiting2007strictly;@carvalho2016overview] such as the Continuous Ranked Probability Score (CRPS) and Weighted Interval Score (WIS). These are available via the `scoringutils::score()` function.\n\nThe `scoringutils::score()` function requires a dataset that contains at least the following columns:\n\n- `date`: the date of the forecast\n- `prediction`: forecast values\n- `true_value`: the observed values\n- `quantile`: If evaluating quantiles, the quantile of the forecasted values (e.g., median, lower and upper bounds), in ascending order.\n\nLet's extract and prepare the forecasts and corresponding observed cases for the evaluation.\n\nEpiNow2's `epinow()` function returns a list of model outputs in the raw and several summarised formats for various use cases. In particular, there are time series of \"infections\", \"reported_cases\", \"growth_rate\", and \"R\", all grouped into \"estimate\", \"estimate based on partial data\", and \"forecast\".\n\nHere, we're interested in the \"forecast\" type of the \"reported_cases\" variable, including the median and quantiles (lower and upper bounds).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the forecasts\nforecasts <- out$estimates$summarised[variable == \"reported_cases\"][type == \"forecast\", ] %>% \n    select(-c(mean, sd, variable, strat, type))\n```\n:::\n\n\n\nLet's also extract the corresponding subset of the observed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract observed cases for the same period\nobs_data <- example_confirmed[date >= min(forecasts$date) & date <= max(forecasts$date)]\n```\n:::\n\n\n\nNow, let's combine the extracted forecasts with the observed data and clean up the quantiles into a format suitable for use with `scoringutils`. We will be cleaning them up because the `forecasts` data.table labels the quantiles as `lower_20`, `upper_20`, etc., where the number indicates the quantile level (e.g., 50% for the median), but that is not compatible with `scoringutils`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine forecasts with observed cases\neval_dt <- merge(forecasts, obs_data, by = \"date\")[, true_value := confirm][, confirm := NULL]\n\n# Melt the data to long format\ncols_to_melt <- c(\"median\", grep(\"^(lower|upper)_\", names(eval_dt), value = TRUE))\neval_long <- melt(\n    eval_dt,\n    id.vars = setdiff(names(eval_dt), cols_to_melt),\n    measure.vars = cols_to_melt,\n    variable.name = \"prediction\",\n    value.name = \"value\"\n)\n\n# Prepare the evaluation data\neval_long[, quantile := fifelse(\n  prediction == \"median\", 0.5,\n  fifelse(\n    grepl(\"^lower_\", prediction),\n    (1 - as.numeric(sub(\"lower_\", \"\", prediction)) / 100) / 2,\n    fifelse(\n      grepl(\"^upper_\", prediction),\n      (1 + as.numeric(sub(\"upper_\", \"\", prediction)) / 100) / 2,\n      NA_real_\n    )\n  )\n)][, prediction := value][, value := NULL]\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in fifelse(grepl(\"^lower_\", prediction), (1 - as.numeric(sub(\"lower_\",\n: NAs introduced by coercion\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in fifelse(grepl(\"^upper_\", prediction), (1 + as.numeric(sub(\"upper_\",\n: NAs introduced by coercion\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sort the data by quantile\nsetorder(eval_long, quantile) %>% \n    head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          date true_value prediction quantile\n        <Date>      <num>      <num>    <num>\n 1: 2020-04-22       2729    1526.00     0.05\n 2: 2020-04-23       3370    1623.90     0.05\n 3: 2020-04-24       2646    1911.95     0.05\n 4: 2020-04-25       3021    1599.00     0.05\n 5: 2020-04-26       2357    1701.95     0.05\n 6: 2020-04-27       2324    1507.00     0.05\n 7: 2020-04-28       1739    1237.85     0.05\n 8: 2020-04-22       2729    1895.00     0.25\n 9: 2020-04-23       3370    2056.75     0.25\n10: 2020-04-24       2646    2420.00     0.25\n```\n\n\n:::\n:::\n\n\n\nNow we can use the `scoringutils` package to compute and summarise the scores. By default, a range of scores will be computed, including the interval_score, dispersion, underprediction, overprediction, coverage_deviation,  bias, and ae_median.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mainly uses the scoringutils package\nscored_scores <- eval_long[, quantile_level := quantile][, quantile := NULL][, model := \"EpiNow2\"] |> # align names to scoringutils preference\n    as_forecast_quantile(observed  = \"true_value\", predicted = \"prediction\") |> \n    score() %>%\n    summarise_scores()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: ! Computation for `interval_coverage_90` failed. Error: ! To compute the\n  interval coverage for an interval range of \"90%\", the 0.05 and 0.95 quantiles\n  are required.\n```\n\n\n:::\n\n```{.r .cell-code}\nknitr::kable(scored_scores, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|model   |     wis| overprediction| underprediction| dispersion|   bias| interval_coverage_50| ae_median|\n|:-------|-------:|--------------:|---------------:|----------:|------:|--------------------:|---------:|\n|EpiNow2 | 313.898|         33.963|         150.892|    129.043| -0.143|                0.571|   444.286|\n\n\n:::\n:::\n\n\n\n## Interpretation of scores\n\nThe table above provides a summary of the forecast performance by metrics. Note that it is not difficult to interpret the scores currently as they are not relative to other models. A proper comparison can only be done against other models.\n\n::: {.callout-tip}\n\n### Comparing against a baseline model\n\nA common practice is to have a baseline model [@stapper2025mind], such as a naive model, and compare the scores of your model against it.\n:::\n\n## Future updates\n\nIn future updates, I will explore more advanced techniques for model diagnostics and evaluation, including posterior predictive checks, cross-validation, and more using packages like [`loo`](https://mc-stan.org/loo/), [shinystan](https://mc-stan.org/shinystan/), and `bayesplot`\n\n## Resources\n\nAs this is a simple and basic guide, I'll direct you to the following resources for more in-depth information on model diagnostics and forecast evaluation.\n\n- Stan's guide on [How to Diagnose and Resolve Convergence Problems](https://mc-stan.org/learn-stan/diagnostics-warnings.html)\n- [Understanding, evaluating, and improving forecasts of infectious disease burden](nfidd.github.io/ueifid/) open course.\n- scoringutils R package [documentation](https://epiforecasts.io/scoringutils/)\n- scoringRules R package [documentation](https://cran.r-project.org/web/packages/scoringRules/vignettes/article.pdf)\n\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}