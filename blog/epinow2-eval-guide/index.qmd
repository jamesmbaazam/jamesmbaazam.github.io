---
title: "How to Validate Time-Varying Reproduction Number Estimation in EpiNow2: MCMC Diagnostics, Convergence, and Forecast Evaluation"
author:
  - name: "James Mba Azam"
  - orcid: "0000-0001-5782-7330"
bibliography: index.bib
date: "2025-11-08"
format:
  html:
    toc: true
execute:
  eval: true
  cache: true
editor_options:
  chunk_output_type: console
freeze: true
categories: [forecasting, EpiNow2, Bayesian Analysis, Reproduction numbers, R, Stan]
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    message = FALSE,
    cache = TRUE
)
set.seed(1234)
```

```{r load-packages}
library(EpiNow2)
library(posterior)
library(bayesplot)
library(data.table)
library(scoringutils)
library(dplyr)
library(ggplot2)
```

## Introduction

This is a comprehensive guide on validating time-varying reproduction number (Rt) estimates from EpiNow2 model runs. EpiNow2 is a Bayesian framework for estimating Rt—a key epidemiological metric indicating disease transmission dynamics—and generating forecasts of case counts. Rigorous validation of these estimates is essential for reliable inference and evidence-based decision-making in infectious disease surveillance and outbreak response.

::: {.callout-note}
### What is the Reproduction Number (Rt)?

The **time-varying reproduction number** (Rt) represents the average number of secondary infections caused by an infected individual at time t. It's a critical metric for understanding epidemic dynamics:

- **Rt > 1**: Epidemic is growing (each case generates more than one new case)
- **Rt = 1**: Epidemic is stable (endemic equilibrium)
- **Rt < 1**: Epidemic is declining (heading toward extinction)

Unlike the basic reproduction number (R0), which assumes a fully susceptible population, Rt accounts for changing conditions like interventions, behavior changes, and population immunity.
:::

::: {.callout-tip collapse="true"}

### EpiNow2 uses a [Stan](https://mc-stan.org/) backend

`{EpiNow2}` uses [Stan](https://mc-stan.org/) in the backend for model specification, fitting, and inference.

This means that the model fit can be assessed using the same tools and techniques used for any Stan model. The EpiNow2 package provides a convenient interface to run the models, but the underlying model is a Stan model.

The returned `fit` object is either a [`<stanfit>`](https://mc-stan.org/rstan/reference/stanfit-class.html) or [`CmdStanModel`](https://mc-stan.org/cmdstanr/reference/CmdStanModel.html) object, depending on the backend ([`{rstan}`](https://mc-stan.org/rstan/index.html) or [`{cmdstanr}`](https://mc-stan.org/cmdstanr/index.html)) used. For example, you can use the [`{bayesplot}`](https://mc-stan.org/bayesplot/reference/bayesplot-package.html) and [`{posterior}`](https://mc-stan.org/posterior/) packages to visualize and summarize the model diagnostics.
:::

Validating EpiNow2 Rt estimates requires a three-pillar approach: (1) **MCMC diagnostics** to ensure the Bayesian sampler has converged and is sampling efficiently from the posterior distribution, (2) **convergence checks** to verify reliable parameter estimates (via metrics like Rhat and effective sample size), and (3) **forecast evaluation** to assess predictive performance against observed data using proper scoring rules. This guide demonstrates each component using the Stan ecosystem's diagnostic tools (`bayesplot`, `posterior`) and the `scoringutils` package for forecast scoring.

We'll demonstrate this workflow by running an example EpiNow2 model for Rt estimation, examining MCMC diagnostics and convergence metrics, and evaluating forecast performance using proper scoring rules.

## Demonstration: EpiNow2 Model Setup and Execution

### Setup: Data and epidemiological parameters

To estimate time-varying Rt, EpiNow2 requires:
1. **Case data**: Time series of reported cases
2. **Epidemiological delays**: Generation time, incubation period, and reporting delays
3. **Prior distributions**: Initial beliefs about Rt and other parameters

Let's set up these components:

```{r model-setup}
# Configure parallel processing for faster MCMC sampling
# Use at most 4 cores, leaving 1 core free for system operations
options(mc.cores = min(4, parallel::detectCores() - 1))

# Define epidemiological delay distributions
# In practice, source these from literature or estimate from data

# Generation time: time between successive infections in a transmission chain
# Using Gamma distribution with uncertainty in parameters
generation_time <- Gamma(
    shape = Normal(1.3, 0.3),    # Mean of ~3.5 days
    rate = Normal(0.37, 0.09),   # Moderate spread
    max = 14                      # Maximum generation interval
)

# Incubation period: time from infection to symptom onset
# Using LogNormal distribution (common for incubation periods)
incubation_period <- LogNormal(
    meanlog = Normal(1.6, 0.06), # Median ~5 days
    sdlog = Normal(0.4, 0.07),   # Variability in incubation
    max = 14                      # Maximum incubation period
)

# Reporting delay: time from symptom onset to case reporting
# Accounts for testing, lab processing, and reporting lags
reporting_delay <- LogNormal(mean = 2, sd = 1, max = 10)

# Load case data
# Using first 60 days of example data to demonstrate workflow
# In practice, use your complete outbreak data
reported_cases <- example_confirmed[1:60]

# Visualize the input data
cases_plots <- ggplot(reported_cases, aes(x = date, y = confirm)) +
    geom_col(fill = "steelblue", alpha = 0.7) +
    labs(
      title = "Confirmed Cases: Input Data",
      x = "Date",
      y = "Reported Cases"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(face = "bold"))

cases_plots
```

### Running the Rt estimation

Now we fit the EpiNow2 model to estimate time-varying Rt and generate forecasts:

```{r run-epinow}
out <- epinow(
    data = reported_cases,
    generation_time = gt_opts(generation_time),
    # Prior on initial Rt: LogNormal(mean=2, sd=0.1) suggests Rt ≈ 2
    rt = rt_opts(prior = LogNormal(mean = 2, sd = 0.1)),
    # Combine incubation and reporting delays
    delays = delay_opts(incubation_period + reporting_delay)
)

# Visualize Rt estimates and forecasts
# This plot shows: estimated Rt over time, case nowcasts, and forecasts
plot(out)
```

The plot above shows three key outputs:
1. **Estimated Rt trajectory**: How transmission intensity changed over time
2. **Infections by date of infection**: Nowcast accounting for reporting delays
3. **Forecast**: Predicted future cases with uncertainty intervals

## Step 1: MCMC Diagnostics and Convergence Checks

Before trusting our Rt estimates, we must verify that the Bayesian MCMC sampler converged properly. EpiNow2 uses Stan for inference, giving us access to Stan's comprehensive diagnostic toolkit via the `bayesplot` and `posterior` packages.

### Key diagnostic metrics

We evaluate convergence using several complementary diagnostics:

- **Divergent transitions**. These should be minimized; ideally 0. Divergent transitions can be improved by tuning Stan controls like `adapt_delta`, `max_treedepth`, and `stepsize` in `stan_opts()`. If there are divergent transitions, increase `adapt_delta` (e.g., 0.95 or 0.99). See an in‑depth explanation at <https://mc-stan.org/learn-stan/diagnostics-warnings.html>.
- **Rhat**. These should be close to 1 indicating good mixing. Rhat values should be less than 1.05 (See `?rstan::Rhat`).
- **Treedepth**. This should be low; high values indicate potential issues with the model.
- **ESS values**. These should be high; low values indicate insufficient sampling. In particular, both bulk‑ESS and tail‑ESS should be at least ~100 per chain to ensure reliable posterior quantile estimates (see `?rstan::ess_bulk`).

```{r diagnostics}
# Extract the Stan fit object from EpiNow2 output
fit <- out$estimates$fit

# Extract NUTS sampler parameters (No-U-Turn Sampler - Stan's HMC variant)
np <- bayesplot::nuts_params(fit)
divergence_data <- subset(np, Parameter == "divergent__")  # Sampling failures
treedepth_data <- subset(np, Parameter == "treedepth__")    # Trajectory lengths

# Compute convergence metrics for all parameters
# Rhat: measures between-chain vs within-chain variance
# ESS: effective sample size (accounts for autocorrelation)
posterior_summary <- posterior::summarize_draws(
    fit, c(posterior::default_convergence_measures(), "ess_basic")
) |> subset(variable != "lp__")  # Exclude log-probability

# Extract minimum ESS values across all parameters
# Worst-case ESS determines overall inference reliability
fit_ess_basic <- min(posterior_summary$ess_basic, na.rm = TRUE)
fit_ess_bulk <- min(posterior_summary$ess_bulk, na.rm = TRUE)
fit_ess_tail <- min(posterior_summary$ess_tail, na.rm = TRUE)

# Compile diagnostic summary table
diagnostics <- data.table(
    "samples" = nrow(np) / length(unique(np$Parameter)),  # Total post-warmup samples
    "max_rhat" = round(max(posterior_summary$rhat, na.rm = TRUE), 3),  # Worst Rhat
    "divergent_transitions" = sum(divergence_data$Value),  # Count of divergences
    "per_divergent_transitions" = mean(divergence_data$Value),  # Proportion divergent
    "max_treedepth" = max(treedepth_data$Value),  # Maximum tree depth reached
    "ess_basic" = fit_ess_basic,  # Basic ESS
    "ess_bulk" = fit_ess_bulk,    # ESS for bulk of distribution
    "ess_tail" = fit_ess_tail     # ESS for distribution tails
)

# Calculate tree depth saturation
# High proportion indicates sampler hitting computational limits
diagnostics[, no_at_max_treedepth :=
                sum(treedepth_data$Value == max_treedepth)
][, per_at_max_treedepth := no_at_max_treedepth / samples]

knitr::kable(diagnostics)
```

### Interpreting the diagnostics

The diagnostics table above summarizes key MCMC performance metrics. For reliable Rt estimates, we should see:

- **max_rhat < 1.05**: Our example shows good convergence (Rhat values close to 1.0)
- **divergent_transitions = 0**: No divergent transitions is ideal. Any divergences suggest the sampler struggled with the posterior geometry
- **ESS values > 400** (for 4 chains × 1000 samples): Bulk-ESS and tail-ESS should be at least 100 per chain. Higher is better for reliable inference
- **per_at_max_treedepth < 0.01**: Few samples hitting maximum tree depth indicates efficient sampling

Based on these criteria, we can assess whether our Rt estimates are trustworthy.

### Visual MCMC diagnostics

While numerical summaries are useful, visual diagnostics provide deeper insight into sampler behavior. Let's examine trace plots and rank plots for key parameters:

```{r visual-diagnostics, fig.height=8, fig.width=10}
# Extract key Rt parameters for visualization
# Using a subset of timepoints for clarity
library(bayesplot)
color_scheme_set("mix-blue-red")

# Trace plots show MCMC sampling behavior over iterations
# Good mixing appears as "fuzzy caterpillars" with chains overlapping
mcmc_trace(fit, pars = c("R[1]", "R[15]", "R[30]", "R[45]"),
           facet_args = list(ncol = 2, strip.position = "left"))
```

```{r rank-plots, fig.height=6, fig.width=10}
# Rank plots check for uniform distribution of ranks across chains
# Deviations from uniform indicate convergence issues
mcmc_rank_overlay(fit, pars = c("R[1]", "R[15]", "R[30]", "R[45]"))
```

Well-mixed chains should show:
- **Trace plots**: Overlapping "fuzzy caterpillar" patterns with no trends or stuck chains
- **Rank plots**: Uniform distribution of ranks across all chains (no systematic patterns)

::: {.callout-warning}
### What if diagnostics fail?

If diagnostics reveal problems (high Rhat, low ESS, divergences):

1. **Increase sampling**: Use `stan_opts(chains = 4, iter_sampling = 4000)` for more samples
2. **Adjust sampler settings**: Increase `adapt_delta` (e.g., 0.95 or 0.99) to reduce divergences
3. **Check model specification**: Ensure delay distributions and priors are reasonable
4. **Increase max_treedepth**: Use `stan_opts(max_treedepth = 12)` if hitting limits
5. **Consult Stan diagnostics guide**: <https://mc-stan.org/misc/warnings.html>

**Do not use** Rt estimates with failing diagnostics—they may be severely biased!
:::

## Step 2: Forecast Performance Evaluation

Forecast performance can be evaluated by comparing the forecast of reported cases with the observed cases using Proper Scoring Rules[@gneiting2007strictly;@carvalho2016overview] such as the Continuous Ranked Probability Score (CRPS) and Weighted Interval Score (WIS). These are available via the `scoringutils::score()` function.

The `scoringutils::score()` function requires a dataset that contains at least the following columns:

- `date`: the date of the forecast
- `prediction`: forecast values
- `true_value`: the observed values
- `quantile`: If evaluating quantiles, the quantile of the forecasted values (e.g., median, lower and upper bounds), in ascending order.

Let's extract and prepare the forecasts and corresponding observed cases for the evaluation.

EpiNow2's `epinow()` function returns a list of model outputs in the raw and several summarised formats for various use cases. In particular, there are time series of "infections", "reported_cases", "growth_rate", and "R", all grouped into "estimate", "estimate based on partial data", and "forecast".

Here, we're interested in the "forecast" type of the "reported_cases" variable, including the median and quantiles (lower and upper bounds).

```{r extract-forecasts}
# Extract the forecasts
forecasts <- out$estimates$summarised[variable == "reported_cases"][type == "forecast", ] %>% 
    select(-c(mean, sd, variable, strat, type))
```

Let's also extract the corresponding subset of the observed data.
```{r evaluate-forecast}
# Extract observed cases for the same period
obs_data <- example_confirmed[date >= min(forecasts$date) & date <= max(forecasts$date)]
```

### Visualizing forecasts against observations

Before scoring, let's visualize how well our forecasts align with observed data:

```{r forecast-plot, fig.height=6, fig.width=10}
# Prepare data for plotting
plot_data <- merge(forecasts, obs_data, by = "date", all.x = TRUE)

# Create forecast vs observed plot
ggplot(plot_data, aes(x = date)) +
  # Prediction intervals (90%, 50%, 20%)
  geom_ribbon(aes(ymin = lower_90, ymax = upper_90), alpha = 0.2, fill = "steelblue") +
  geom_ribbon(aes(ymin = lower_50, ymax = upper_50), alpha = 0.3, fill = "steelblue") +
  geom_ribbon(aes(ymin = lower_20, ymax = upper_20), alpha = 0.4, fill = "steelblue") +
  # Median forecast
  geom_line(aes(y = median), color = "steelblue", size = 1) +
  # Observed data
  geom_point(aes(y = confirm), color = "black", size = 2) +
  geom_line(aes(y = confirm), color = "black", linetype = "dashed") +
  labs(
    title = "EpiNow2 Forecast vs Observed Cases",
    subtitle = "Shaded regions show 20%, 50%, and 90% prediction intervals",
    x = "Date",
    y = "Reported Cases"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"))
```

Ideally, observed values (black points) should fall mostly within the prediction intervals, especially the wider ones. Systematic deviations suggest model miscalibration.

### Preparing data for scoring

Now, let's combine the extracted forecasts with the observed data and clean up the quantiles into a format suitable for use with `scoringutils`. We will be cleaning them up because the `forecasts` data.table labels the quantiles as `lower_20`, `upper_20`, etc., where the number indicates the quantile level (e.g., 50% for the median), but that is not compatible with `scoringutils`.

```{r combine-forecasts-obs}
# Combine forecasts with observed cases
eval_dt <- merge(forecasts, obs_data, by = "date")[, true_value := confirm][, confirm := NULL]

# Melt the data to long format
cols_to_melt <- c("median", grep("^(lower|upper)_", names(eval_dt), value = TRUE))
eval_long <- melt(
    eval_dt,
    id.vars = setdiff(names(eval_dt), cols_to_melt),
    measure.vars = cols_to_melt,
    variable.name = "prediction",
    value.name = "value"
)

# Prepare the evaluation data
eval_long[, quantile := fifelse(
  prediction == "median", 0.5,
  fifelse(
    grepl("^lower_", prediction),
    (1 - as.numeric(sub("lower_", "", prediction)) / 100) / 2,
    fifelse(
      grepl("^upper_", prediction),
      (1 + as.numeric(sub("upper_", "", prediction)) / 100) / 2,
      NA_real_
    )
  )
)][, prediction := value][, value := NULL]

# Sort the data by quantile
setorder(eval_long, quantile) %>% 
    head(10)
```

Now we can use the `scoringutils` package to compute and summarise the scores. By default, a range of scores will be computed, including the interval_score, dispersion, underprediction, overprediction, coverage_deviation,  bias, and ae_median.

```{r scoring}
# Mainly uses the scoringutils package
scored_scores <- eval_long[, quantile_level := quantile][, quantile := NULL][, model := "EpiNow2"] |> # align names to scoringutils preference
    as_forecast_quantile(observed  = "true_value", predicted = "prediction") |> 
    score() %>%
    summarise_scores()
knitr::kable(scored_scores, digits = 3)
```

## Interpretation of scores

The table above provides a summary of the forecast performance across multiple metrics. However, interpreting these scores in isolation can be challenging, as they are not relative to other models. Proper scoring rules are most informative when comparing multiple models or against baseline forecasts.

Let's understand what each metric tells us:

- **interval_score**: Overall forecast accuracy combining sharpness (narrow intervals) and calibration (actual coverage). Lower is better.
- **dispersion**: Measures the width of prediction intervals. Lower values indicate sharper (more confident) forecasts.
- **underprediction** and **overprediction**: Penalties for forecasts that systematically miss below or above observations.
- **coverage_deviation**: Difference between nominal and empirical coverage of prediction intervals. Values near zero indicate well-calibrated intervals.
- **bias**: Systematic tendency to over- or under-forecast. Values near zero are ideal.
- **ae_median**: Absolute error of the median forecast. A simple point forecast accuracy metric.

For this example run, well-performing models should show low interval scores, minimal bias, and coverage deviation close to zero

::: {.callout-tip}

### Comparing against a baseline model

A common practice is to have a baseline model [@stapper2025mind], such as a naive model, and compare the scores of your model against it.
:::

## Conclusion: Validation Checklist

We've demonstrated a comprehensive three-pillar approach to validating EpiNow2 Rt estimates. Use this checklist to assess whether your model results are reliable:

### ✓ MCMC Diagnostics Pass
- [ ] **Rhat < 1.05** for all parameters (ideally < 1.01)
- [ ] **Zero divergent transitions** (or < 1% if unavoidable)
- [ ] **ESS bulk and tail > 400** total (>100 per chain minimum)
- [ ] **Low tree depth saturation** (< 1% hitting max_treedepth)
- [ ] **Trace plots** show good mixing (fuzzy caterpillars)
- [ ] **Rank plots** show uniform distribution

### ✓ Forecast Evaluation Complete
- [ ] **Visual inspection** shows forecasts align with observations
- [ ] **Prediction intervals** contain observed values at appropriate rates
- [ ] **Scoring metrics** calculated (interval score, bias, coverage)
- [ ] **Comparison to baseline** (if available) shows improvement

### ✓ Interpretation and Documentation
- [ ] Rt trajectory makes epidemiological sense
- [ ] Uncertainty intervals are appropriate (not too narrow/wide)
- [ ] Model assumptions documented (delays, priors)
- [ ] Limitations acknowledged

**When to trust your Rt estimates**: Only when all diagnostic checks pass and forecast evaluation is satisfactory. Failing diagnostics or poor forecast performance indicate model issues that must be resolved before drawing conclusions.

**When to seek help**: If diagnostics consistently fail despite tuning, or forecasts systematically miss observations, consult the Stan community forums or EpiNow2 GitHub discussions.

## Future updates

In future updates, I will explore more advanced techniques for model diagnostics and evaluation, including posterior predictive checks, cross-validation, and more using packages like [`loo`](https://mc-stan.org/loo/), [shinystan](https://mc-stan.org/shinystan/), and `bayesplot`

## Resources

For more in-depth information on advanced model diagnostics and forecast evaluation techniques, I recommend the following resources:

### MCMC Diagnostics and Stan

- **[Stan Diagnostics Guide](https://mc-stan.org/learn-stan/diagnostics-warnings.html)**: Official guide on diagnosing and resolving MCMC convergence issues, including detailed explanations of divergences, Rhat, and ESS.

- **[bayesplot Visual MCMC Diagnostics](https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html)**: Comprehensive tutorial on using trace plots, rank plots, and other visual diagnostics to assess MCMC performance.

- **[Posterior Predictive Checks](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html)**: Guide to validating model fit by comparing simulated data from the posterior to observed data.

### Forecast Evaluation

- **[Understanding, Evaluating, and Improving Forecasts of Infectious Disease Burden](https://nfidd.github.io/ueifid/)**: Open course covering forecast methodology, proper scoring rules, and evaluation best practices specific to infectious disease applications.

- **[scoringutils R package](https://epiforecasts.io/scoringutils/)**: Documentation for computing and visualizing forecast scores (CRPS, WIS, interval score, etc.) with practical examples.

- **[scoringRules R package](https://cran.r-project.org/web/packages/scoringRules/vignettes/article.pdf)**: Theoretical foundations and implementations of proper scoring rules for probabilistic forecasts.

### EpiNow2 Documentation

- **[EpiNow2 package website](https://epiforecasts.io/EpiNow2/)**: Official documentation with function references, vignettes, and usage examples.

- **[EpiNow2 GitHub](https://github.com/epiforecasts/EpiNow2)**: Source code, issue tracker, and community discussions for troubleshooting and feature requests.


## References
